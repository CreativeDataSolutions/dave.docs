{"title":"Predictive modeling","markdown":{"yaml":{"format":{"html":{"toc":true,"toc-title":"On this page","toc-depth":4,"number-sections":false,"anchor-sections":true,"smooth-scroll":true,"css":"../style.css"}}},"headingText":"Predictive modeling","containsRefs":false,"markdown":"\n\n```{r, echo=FALSE,warning=FALSE,message=FALSE}\nsuppressPackageStartupMessages(library('dave.help'))\n```\n\n\n### [Model]{.txbox_green}\n##### Create and optimize machine learning models for classification and regression tasks.\n\n#### [Calculate]{.txbox_gray}\n\n::::: columns \n::: column \n\n##### Selecting the type of model you would like to create `classification` or `regression` will toggle the available variables to predict. Use the filter menu `selected` to specify which variables should be included in the model based on criteria generated in other modules (e.g. statistical and feature selection).\n\n::: \n\n::: column \n![](img/dave.ml/model/data_menue.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### The `model` menu is used to specify a single or multiple models to fit to the data. The model hyperparameters can be automatically tuned based on `grid size` which defines the number or random setting to test. Specific model hyperparameters can be specified using the `manual` setting. Note when fitting multiple models tuning will be done in the `auto` mode. The `optimize for` is used to specify if you want to select the best model based on performance on the `training` (recommended) or held out `test` data.\n::: \n\n::: column \n![](img/dave.ml/model/model_menue.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### This menu is used to specify the `training` and `test` data and model cross-validation parameters. Similar to the `model>>tune` the cross-validation parameters can be `manually` or `automatically` set. For example, the selections shown in the example will randomly select 70% of the data (samples) to fit the model which will then be validated on the 305 held out or `test` data. The model will be internally cross-validated by splitting the `training` data into `7` folds and then leaving out each fold during the model fit and then testing the performance on the held-out fold. This process will be repeated `3` times and the performance on the `training` data will be summarized over all the results.\n\n::: \n\n::: column \n![](img/dave.ml/model/train_menue.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### Model methods amd performance summary. For eaxample, this show three models were fitted, RandomForest (rf), Partial Least Squares projections to latent structures (pls) and radial kernel Support Vectoem Machine (svmRadial). The top performing model (rf) is highlighted in green.\n\n::: \n\n::: column \n![](img/dave.ml/model/results.png){.bordered}\n:::\n:::\n\n#### [Plot and Explore]{.txbox_gray}\n\n::::: columns \n::: column \n\n##### Model performance for the `training` and `test` data and training time can be compared. The `y-axis` shows the selected model performance metric and `x-axis` the training time.\n\n::: \n\n::: column \n![](img/dave.ml/model/perf_plot.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### This plot is used to visualize the impact of hyperparameters on model performance.\n\n::: \n\n::: column \n![](img/dave.ml/model/model_plot.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### Identify the proportion of miss classified samples for classification models using a `confusion matrix`. Optionally show actual counts or percent for correct and incorrect classifications.\n\n::: \n\n::: column \n![](img/dave.ml/model/confusion_plot.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### Visualize variable's importance or contribution to the model's performance. Importance for multiple models is calculated based weighted metric of the modelâ€™s performance and each variables importance in the model. Importance based on multiple models displays the variables consensus rank (y-axis) across all models and the actual importance in the single highest performing model (x-axis).\n\n::: \n\n::: column \n![](img/dave.ml/model/importance_plot.png){.bordered}\n:::\n:::\n\n### [Feature selection]{.txbox_green}\n\n##### `Feature selection` is used to identify variables which maximize model performance. Optimal variables are identified using recursive feature elimination wherein many models are built from subsets of variables and an optimal model is identified based on which subset yielded the highest performing model.\n\n#### [Calculate]{.txbox_gray}\n\n::::: columns \n::: column \n\n##### The `data` menu is used to specify the model type and select target and predictor variables.\n\n::: \n\n::: column \n![](img/dave.ml/select/data_menu.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n##### The `optimize` menu is used to specify the algorithm used for the selection. The metric specifies which performance criteria will be used to identify the optimal subset of variables.\n::: \n\n::: column \n![](img/dave.ml/select/optimize_menu.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### The `validate` menu is used to specify the model cross-validation parameters and size of the automatic hyperparameter tuning grid.\n\n::: \n\n::: column \n![](img/dave.ml/select/validate_menu.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### View feature selection methods and results.\n::: \n\n::: column \n![](img/dave.ml/select/results.png){.bordered}\n:::\n:::\n\n#### [Plot and Explore]{.txbox_gray}\n\n::::: columns \n::: column \n\n##### This visualization displays model performance (y-axis) based on the subset of variables (x-axis). The optimal model is highlighted in red. The plot controls can be used to specify which model metric will be used for the visualization (use `calculate` to optimized subsets for that metric). The optimal variables can be selected based on the `subset function`. Options include `PickSizeBest` which specifies the subset which maximized or minimized the chosen performance metric or `PickSizeTolerance` which allows for models with less parameters (variables), which are also worse than the optimal model. The accepted decrease in performance is specified as a percent of the metric in `tolerance`.\n::: \n\n::: column \n![](img/dave.ml/select/overall_plot.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n\n##### This visualizations shows the selected variables (red) importance compared to those which were removed (blue).\n::: \n\n::: column \n![](img/dave.ml/select/importance_plot.png){.bordered}\n:::\n:::\n\n::::: columns \n::: column \n##### Add selected features filter to the `row_metadata` or remove all non-selected variables from the data set `keep selected`. Common workflows might include `feature selection` followed by `training` wherein the `rfe_selected` filter can be used to select variables in the `model >> data >> filter >> selected` menu.\n::: \n\n::: column \n![](img/dave.ml/select/save.png){.bordered}\n:::\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"css":["../style.css"],"output-file":"model.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.38","bibliography":["../references.bib"],"theme":"darkly","toc-title":"On this page","anchor-sections":true,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"model.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{}}}}}